{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import GRU\n",
    "from keras.optimizers import Adamax\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ghazals=pd.read_excel('urdu/faiz/ghazals.xlsx')\n",
    "ghazals=ghazals.reset_index()\n",
    "del ghazals['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Because i want it to be purly urdu so i'll delete all the nazam's with english\n",
    "\n",
    "ghazals_pruned=pd.DataFrame(columns=list(['title','text']))\n",
    "for counter,i in enumerate(ghazals['text']):\n",
    "    if(re.search('[a-zA-Z]', i))==None:\n",
    "        df = pd.DataFrame([[ghazals['title'][counter], i]],columns=list(['title','text']))\n",
    "        ghazals_pruned=ghazals_pruned.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest line in the literature 63\n"
     ]
    }
   ],
   "source": [
    "#Finding the longest line in literature\n",
    "longest=0\n",
    "for i in ghazals_pruned['text']:\n",
    "    #Finding the longest line in the literature\n",
    "    for j in i.split('\\n'):\n",
    "        if longest<len(j):\n",
    "            longest=len(j)\n",
    "print(\"Longest line in the literature\",longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the literature: 36245\n"
     ]
    }
   ],
   "source": [
    "text=''\n",
    "for i in ghazals_pruned['text']:\n",
    "    #Creating the dataset\n",
    "    text=text+i\n",
    "print(\"Length of the literature:\",len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text=''\n",
    "for i in ghazals_pruned['text']:\n",
    "    #Finding the longest line in the literature\n",
    "    for j in i.split('\\n'):\n",
    "        if(len(j)<=2):\n",
    "            continue\n",
    "        j=j.strip()\n",
    "        j=j.ljust(longest-1, ' ')\n",
    "        j=j+'\\n'\n",
    "        new_text=new_text+j\n",
    "ghazals_final=new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ghazals_final = ghazals_final.replace(\"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nazams=pd.read_excel('urdu/faiz/nazams.xlsx')\n",
    "nazams=nazams.reset_index()\n",
    "del nazams['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Because i want it to be purly urdu so i'll delete all the literature with english\n",
    "\n",
    "nazams_pruned=pd.DataFrame(columns=list(['title','text']))\n",
    "for counter,i in enumerate(nazams['text']):\n",
    "    if(re.search('[a-zA-Z]', i))==None:\n",
    "        df = pd.DataFrame([[nazams['title'][counter], i]],columns=list(['title','text']))\n",
    "        nazams_pruned=nazams_pruned.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nazams_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Finding the longest line in literature\n",
    "# longest=0\n",
    "# for i in nazams_pruned['text']:\n",
    "#     #Finding the longest line in the literature\n",
    "#     for j in i.split('\\n'):\n",
    "#         if longest<len(j):\n",
    "#             longest=len(j)\n",
    "# print(\"Longest line in the literature\",longest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the literature: 52424\n"
     ]
    }
   ],
   "source": [
    "text=''\n",
    "for i in nazams_pruned['text']:\n",
    "    #Creating the dataset\n",
    "    text=text+i\n",
    "print(\"Length of the literature:\",len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_text=''\n",
    "for i in nazams_pruned['text']:\n",
    "    #Finding the longest line in the literature\n",
    "    for j in i.split('\\n'):\n",
    "        if(len(j)<=2):\n",
    "            continue\n",
    "        j=j.strip()\n",
    "        j=j.ljust(longest-1, ' ')\n",
    "        j=j+'\\n'\n",
    "        new_text=new_text+j\n",
    "nazams_final=new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#some preprocessing\n",
    "nazams_final = nazams_final.replace(\"'\", \"\")\n",
    "nazams_final = nazams_final.replace(\"(1)\", \"\")\n",
    "nazams_final = nazams_final.replace(\"(۲)\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 52\n"
     ]
    }
   ],
   "source": [
    "#Finding unique characters\n",
    "chars = sorted(list(set(nazams_final+ghazals_final)))\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '،',\n",
       " 'ؔ',\n",
       " '؟',\n",
       " 'ء',\n",
       " 'آ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'ا',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'و',\n",
       " 'ٓ',\n",
       " 'ٔ',\n",
       " 'ٰ',\n",
       " 'ٹ',\n",
       " 'پ',\n",
       " 'چ',\n",
       " 'ڈ',\n",
       " 'ڑ',\n",
       " 'ک',\n",
       " 'گ',\n",
       " 'ں',\n",
       " 'ھ',\n",
       " 'ہ',\n",
       " 'ۂ',\n",
       " 'ی',\n",
       " 'ے',\n",
       " '۔']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169462"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ghazals_final+nazams_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_text=ghazals_final+nazams_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = longest\n",
    "step = longest\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters=set()\n",
    "target_characters=set()\n",
    "\n",
    "for i in range(0, len(final_text) - maxlen, step*2):\n",
    "    input_text=final_text[i: i + maxlen]\n",
    "    target_text=final_text[i + maxlen:i+maxlen + maxlen]\n",
    "    target_text='\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 1345\n",
      "Number of unique input tokens: 51\n",
      "Number of unique output tokens: 53\n",
      "Max sequence length for inputs: 63\n",
      "Max sequence length for outputs: 65\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "latent_dim=1024\n",
    "batch_size=64\n",
    "epochs=250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "# encoder = LSTM(latent_dim, return_state=True)(encoder)\n",
    "# encoder = LSTM(latent_dim, return_state=True)(encoder)\n",
    "\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_8 (InputLayer)            (None, None, 51)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_9 (InputLayer)            (None, None, 53)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_9 (LSTM)                   [(None, 1024), (None 4407296     input_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_10 (LSTM)                  [(None, None, 1024), 4415488     input_9[0][0]                    \n",
      "                                                                 lstm_9[0][1]                     \n",
      "                                                                 lstm_9[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, None, 53)     54325       lstm_10[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 8,877,109\n",
      "Trainable params: 8,877,109\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1076 samples, validate on 269 samples\n",
      "Epoch 1/250\n",
      "1076/1076 [==============================] - 12s 11ms/step - loss: 3.0399 - val_loss: 2.0317\n",
      "Epoch 2/250\n",
      "1076/1076 [==============================] - 7s 6ms/step - loss: 1.9662 - val_loss: 1.9011\n",
      "Epoch 3/250\n",
      "1076/1076 [==============================] - 7s 6ms/step - loss: 1.9080 - val_loss: 1.8655\n",
      "Epoch 4/250\n",
      "1076/1076 [==============================] - 7s 6ms/step - loss: 1.8754 - val_loss: 1.8077\n",
      "Epoch 5/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.8093 - val_loss: 1.7091\n",
      "Epoch 6/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.7082 - val_loss: 1.7081\n",
      "Epoch 7/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.6659 - val_loss: 1.7152\n",
      "Epoch 8/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.6547 - val_loss: 1.6488\n",
      "Epoch 9/250\n",
      "1076/1076 [==============================] - 7s 6ms/step - loss: 1.6184 - val_loss: 1.6298\n",
      "Epoch 10/250\n",
      "1076/1076 [==============================] - 7s 6ms/step - loss: 1.5978 - val_loss: 1.6052\n",
      "Epoch 11/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.5841 - val_loss: 1.5875\n",
      "Epoch 12/250\n",
      "1076/1076 [==============================] - 7s 6ms/step - loss: 1.5654 - val_loss: 1.5767\n",
      "Epoch 13/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.5416 - val_loss: 1.5744\n",
      "Epoch 14/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.5348 - val_loss: 1.5685\n",
      "Epoch 15/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.5017 - val_loss: 1.5441\n",
      "Epoch 16/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.4689 - val_loss: 1.5362\n",
      "Epoch 17/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.4545 - val_loss: 1.5044\n",
      "Epoch 18/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.4298 - val_loss: 1.5199\n",
      "Epoch 19/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3974 - val_loss: 1.5248\n",
      "Epoch 20/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3819 - val_loss: 1.5046\n",
      "Epoch 21/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3547 - val_loss: 1.4536\n",
      "Epoch 22/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3388 - val_loss: 1.4637\n",
      "Epoch 23/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3500 - val_loss: 1.4465\n",
      "Epoch 24/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3129 - val_loss: 1.4762\n",
      "Epoch 25/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.3079 - val_loss: 1.4209\n",
      "Epoch 26/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2923 - val_loss: 1.4395\n",
      "Epoch 27/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2712 - val_loss: 1.4875\n",
      "Epoch 28/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2641 - val_loss: 1.4464\n",
      "Epoch 29/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2539 - val_loss: 1.4019\n",
      "Epoch 30/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2458 - val_loss: 1.4274\n",
      "Epoch 31/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2328 - val_loss: 1.4103\n",
      "Epoch 32/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2261 - val_loss: 1.5031\n",
      "Epoch 33/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2261 - val_loss: 1.3948\n",
      "Epoch 34/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2062 - val_loss: 1.3859\n",
      "Epoch 35/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2154 - val_loss: 1.3758\n",
      "Epoch 36/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.2002 - val_loss: 1.3801\n",
      "Epoch 37/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1948 - val_loss: 1.4092\n",
      "Epoch 38/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1861 - val_loss: 1.3776\n",
      "Epoch 39/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1787 - val_loss: 1.4191\n",
      "Epoch 40/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1632 - val_loss: 1.4426\n",
      "Epoch 41/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1615 - val_loss: 1.3880\n",
      "Epoch 42/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1627 - val_loss: 1.3628\n",
      "Epoch 43/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1453 - val_loss: 1.3924\n",
      "Epoch 44/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1543 - val_loss: 1.3549\n",
      "Epoch 45/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1329 - val_loss: 1.3723\n",
      "Epoch 46/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1197 - val_loss: 1.3715\n",
      "Epoch 47/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1232 - val_loss: 1.3486\n",
      "Epoch 48/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1262 - val_loss: 1.3602\n",
      "Epoch 49/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.1070 - val_loss: 1.3719\n",
      "Epoch 50/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0928 - val_loss: 1.3574\n",
      "Epoch 51/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0800 - val_loss: 1.3552\n",
      "Epoch 52/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0820 - val_loss: 1.3719\n",
      "Epoch 53/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0742 - val_loss: 1.3550\n",
      "Epoch 54/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0639 - val_loss: 1.3699\n",
      "Epoch 55/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0578 - val_loss: 1.3651\n",
      "Epoch 56/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0557 - val_loss: 1.3673\n",
      "Epoch 57/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0360 - val_loss: 1.4081\n",
      "Epoch 58/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0399 - val_loss: 1.3655\n",
      "Epoch 59/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0224 - val_loss: 1.3918\n",
      "Epoch 60/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0258 - val_loss: 1.4187\n",
      "Epoch 61/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0118 - val_loss: 1.3905\n",
      "Epoch 62/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 1.0088 - val_loss: 1.3679\n",
      "Epoch 63/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9925 - val_loss: 1.3916\n",
      "Epoch 64/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9859 - val_loss: 1.3979\n",
      "Epoch 65/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9666 - val_loss: 1.4290\n",
      "Epoch 66/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9673 - val_loss: 1.3954\n",
      "Epoch 67/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9524 - val_loss: 1.4384\n",
      "Epoch 68/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9555 - val_loss: 1.4382\n",
      "Epoch 69/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9417 - val_loss: 1.4025\n",
      "Epoch 70/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9272 - val_loss: 1.4464\n",
      "Epoch 71/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9202 - val_loss: 1.4429\n",
      "Epoch 72/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.9051 - val_loss: 1.4859\n",
      "Epoch 73/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8898 - val_loss: 1.4842\n",
      "Epoch 74/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8852 - val_loss: 1.5047\n",
      "Epoch 75/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8898 - val_loss: 1.4542\n",
      "Epoch 76/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8686 - val_loss: 1.4763\n",
      "Epoch 77/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8472 - val_loss: 1.5099\n",
      "Epoch 78/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8509 - val_loss: 1.5097\n",
      "Epoch 79/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8272 - val_loss: 1.5124\n",
      "Epoch 80/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.8123 - val_loss: 1.5405\n",
      "Epoch 81/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7987 - val_loss: 1.5449\n",
      "Epoch 82/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7761 - val_loss: 1.5895\n",
      "Epoch 83/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7624 - val_loss: 1.5879\n",
      "Epoch 84/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7546 - val_loss: 1.6113\n",
      "Epoch 85/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7310 - val_loss: 1.6085\n",
      "Epoch 86/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7138 - val_loss: 1.6298\n",
      "Epoch 87/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7087 - val_loss: 1.6464\n",
      "Epoch 88/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7001 - val_loss: 1.6240\n",
      "Epoch 89/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.7175 - val_loss: 1.6450\n",
      "Epoch 90/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.6800 - val_loss: 1.6609\n",
      "Epoch 91/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.6552 - val_loss: 1.6917\n",
      "Epoch 92/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.6409 - val_loss: 1.7010\n",
      "Epoch 93/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.6234 - val_loss: 1.7161\n",
      "Epoch 94/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.5998 - val_loss: 1.7416\n",
      "Epoch 95/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.5772 - val_loss: 1.8057\n",
      "Epoch 96/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.5601 - val_loss: 1.8223\n",
      "Epoch 97/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.5646 - val_loss: 1.7706\n",
      "Epoch 98/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.5393 - val_loss: 1.8211\n",
      "Epoch 99/250\n",
      "1076/1076 [==============================] - 7s 7ms/step - loss: 0.5152 - val_loss: 1.8861\n",
      "Epoch 100/250\n",
      " 512/1076 [=============>................] - ETA: 3s - loss: 0.4932"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-d5d6617f971d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m           validation_split=0.2)\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "model.compile(optimizer='adamax', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('seq2seq.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "شرح فراق مدح لب مشکبو کریں                                    \n",
      "\n",
      "Decoded sentence: کر باد دلاں کو فری تے ترو گفر کر کر                           \n",
      "\n",
      "-\n",
      "یار آشنا نہیں کوئی ٹکرائیں کس سے جام                         \n",
      "\n",
      "Decoded sentence:  س کو چلب سے کو ترے تر وور کر کریں                            \n",
      "\n",
      "-\n",
      "سینے پہ ہاتھ ہے نہ نظر کو تلاش بام                            \n",
      "\n",
      "Decoded sentence:  س کو چلب سے کو ترے تر وور کر کریں                            \n",
      "\n",
      "-\n",
      "کب تک سنے گی رات کہاں تک سنائیں ہم                            \n",
      "\n",
      "Decoded sentence: کر باد دلاں کو فرض تل گر گرو کریں                             \n",
      "\n",
      "-\n",
      "ہمدم حدیث کوئے ملامت سنائیو                                   \n",
      "\n",
      "Decoded sentence: کر بات  بے آج کے در بر گرفگ رفی                               \n",
      "\n",
      "-\n",
      "آشفتہ سر ہیں محتسبو منہ نہ آئیو                              \n",
      "\n",
      "Decoded sentence: کر باد دلاں کو فری تے تر وفگ عرو کریں                          \n",
      "\n",
      "-\n",
      "تر دامنی پہ شیخ ہماری نہ جائیو                              \n",
      "دا\n",
      "Decoded sentence:  کو چل دوں کے ترف تر گرو گر کر کریں                         \n",
      "\n",
      "-\n",
      "کا پھر کوئی دروازہ کھلا آخر شب                            \n",
      "دل م\n",
      "Decoded sentence: اند کو چلے دی تور آخ  خو شبر آ ر                         \n",
      "\n",
      "-\n",
      "پھوٹی تو وہ پہلو سے اٹھا آخر شب                           \n",
      "وہ ج\n",
      "Decoded sentence: اند کو چلے دی تبھی آخر آر شب                              \n",
      "\n",
      "-\n",
      " سے ماند ستاروں نے کہا آخر شب                             \n",
      "کون \n",
      "Decoded sentence: باتی کو  کا دہ سے آخا آبر آخش شا                           \n",
      "\n",
      "-\n",
      "جانانہ لیے مستیٔ پیمانہ لیے                               \n",
      "حمد \n",
      "Decoded sentence: انداس سے چلا باصصبا آخر شب                               \n",
      "\n",
      "-\n",
      "جو ویراں تھا سر شام وہ کیسے کیسے                          \n",
      "فرقت\n",
      "Decoded sentence:  بار کو چاک دیستا ہے گیر شبر آ ر                           \n",
      "\n",
      "-\n",
      "دا سے کوئی آیا تھا کبھی اول شب                            \n",
      "اسی \n",
      "Decoded sentence: انداس سے چلا باصصبا آخر شب                              \n",
      "\n",
      "-\n",
      "وج در موج غم تھم گیا اس طرح غم زدوں کو قرار آ گیا     \n",
      "جیسے خوش\n",
      "Decoded sentence: ں کوئی یاتوں کی خار کیو کو کا ڈے کے کرا دیا           \n",
      "\n",
      "-\n",
      "د و طلب وہم سمجھے تھے ہم رو بہ رو پھر سر رہ گزار آ گیا\n",
      "صبح فردا\n",
      "Decoded sentence:  کو پھر دل ترسے لگا گمر رفر کر اراتاراب آ گیا           \n",
      "\n",
      "-\n",
      " لگی رنگ دل دیکھنا رنگ گلشن سے اب حال کھلتا نہیں      \n",
      "زخم چھلک\n",
      "Decoded sentence: ا کوئی یا کوئی گل کھلا کا کے ڈام کہ بہر باب آ آ گیا   \n",
      "\n",
      "-\n",
      " سے جام بھرنے لگے دل سلگنے لگے داغ جلنے لگے           \n",
      "محفل درد\n",
      "Decoded sentence:  کوئی تری روں کی خار کی صدا ایزے کبھاباد ہیا         \n",
      "\n",
      "-\n",
      "کے انداز بدلے گئے دعوت قتل پر مقتل شہر میں            \n",
      "ڈال کر ک\n",
      "Decoded sentence: وئی گردی میں وو ہیای ادل کا صدا کیا                   \n",
      "\n",
      "-\n",
      " جانیے یار کس آس پر منتظر ہیں کہ لائے گا کوئی خبر     \n",
      "مے کشوں \n",
      "Decoded sentence: کوئی  گر بھی و گیا یاں کا ابا ہواے کات  کراں آ گیا     \n",
      "\n",
      "-\n",
      "ے کسی جانب تری زیبائی کا                              \n",
      "رنگ بدلے\n",
      "Decoded sentence:  کسی ایراں شب دندائے کا                               \n",
      "\n",
      "-\n",
      "سے پھر اے خسرو شیریں دہناں                            \n",
      "آج ارزاں\n",
      "Decoded sentence:  ہو کوئی تری شنائیاں کا                               \n",
      "\n",
      "-\n",
      "ک سے ہر انجمن گل بدناں                                \n",
      "تذکرہ چھ\n",
      "Decoded sentence: ی ہے کہیں میں خدائیاں کیا کیا                         \n",
      "\n",
      "-\n",
      " میں کبھی اے شہ شمشاد قداں                            \n",
      "پھر نظر \n",
      "Decoded sentence: سے کیا ہم نے داندائی کا                               \n",
      "\n",
      "-\n",
      "اور مسیحائے دل دل زدگاں                               \n",
      "کوئی وعد\n",
      "Decoded sentence: ا کے ہر کی دیکھ نی  کی                                \n",
      "\n",
      "-\n",
      "ل کو سنبھالو کہ سر شام فراق                           \n",
      "ساز و سا\n",
      "Decoded sentence: تی ہے تو سم سے دہ ائی نہیا                            \n",
      "\n",
      "-\n",
      " چھپا کے دیکھ لیا                                     \n",
      "دل بہت ک\n",
      "Decoded sentence: ی ارتاں کی طرح                                        \n",
      "\n",
      "-\n",
      "دیکھنے کو باقی ہے                                     \n",
      "آپ سے دل\n",
      "Decoded sentence:  کا کے دیکھ لیا                                      \n",
      "\n",
      "-\n",
      "و کے بھی مرے نہ ہوئے                                  \n",
      "ان کو اپ\n",
      "Decoded sentence: نہ الگا کی دیکھ لی                                    \n",
      "\n",
      "-\n",
      " نظر میں کچھ ہم نے                                    \n",
      "سب کی نظ\n",
      "Decoded sentence: ر کوئی ایکا کی طرح                                    \n",
      "\n",
      "-\n",
      "یل غم بھی ہو نہ سکی                                   \n",
      "عشق کو آ\n",
      "Decoded sentence: را کے گیاں کی طرح                                     \n",
      "\n",
      "-\n",
      "ائے گا رسم وفا ایسے نہیں ہوتا                         \n",
      "صنم دکھل\n",
      "Decoded sentence:  سانے تلے قادل سم نہ کیا                              \n",
      "\n",
      "-\n",
      "سرتیں جو خوں ہوئی ہیں تن کے مقتل میں                  \n",
      "مرے قاتل\n",
      "Decoded sentence:  ہو کے چھیں جو بائے دوس نا کانے کیا                   \n",
      "\n",
      "-\n",
      "میں کام آتی ہیں تدبیریں نہ تعزیریں                    \n",
      "یہاں پیم\n",
      "Decoded sentence: اں سے بھی چانہ  و  کائے دند کا نے دلا                 \n",
      "\n",
      "-\n",
      " ہر گھڑی گزرے قیامت یوں تو ہوتا ہے                    \n",
      "مگر ہر ص\n",
      "Decoded sentence: وئی ہو بھیٹھ گئی بائ ارک نا  ناز کا کے                \n",
      "\n",
      "-\n",
      "نبض دوراں گردشوں میں آسماں سارے                       \n",
      "جو تم کہ\n",
      "Decoded sentence:  سے کیچھ میں آنائے دانو کیا                           \n",
      "\n",
      "-\n",
      " ان آنکھوں آگے کیا کیا نہ نظارا گزرے تھا              \n",
      "کیا روشن\n",
      "Decoded sentence: ائی ہے ابھی ام آج میں نہم کا   ر                      \n",
      "\n",
      "-\n",
      " اچھے لوگ کہ جن کو اپنے غم سے فرصت تھی                \n",
      "سب پوچھی\n",
      "Decoded sentence: ں کوئی میں کبھی بھی روق آجا کیا   کا کے کبا  دا         \n",
      "\n",
      "-\n",
      "اں ایسی ٹھہری وہ سارے زمانے بھول گئے                  \n",
      "جب موسم \n",
      "Decoded sentence: بات نہیں اوقار سے خاباہ کا ارا                         \n",
      "\n",
      "-\n",
      "ں کی بہتات تو ہم اغیار سے بھی بیزار نہ تھے            \n",
      "جب مل بی\n",
      "Decoded sentence: ں کوئی تلے کبھی بہ  کی صبا آجز کی  ک                  \n",
      "\n",
      "-\n",
      "تھ سجھائی نہ دیوے لیکن اب سے پہلے تو                  \n",
      "آنکھ اٹھ\n",
      "Decoded sentence: ے ہو ال کو کیڑ کی یار یار نگزر گزا                    \n",
      "\n",
      "-\n",
      "ہ تو نے گوشۂ لب اے جان جہاں غماز کیا                  \n",
      "اعلان جن\n",
      "Decoded sentence: ائی ہے ماں بہ جائے میس نا کاز  داز کیا               \n",
      "\n",
      "-\n",
      " تھے پیوست گلو جب چھیڑی شوق کی لے ہم نے               \n",
      "سو تیر ت\n",
      "Decoded sentence: و سار چلے ہی ار بھی باق آرازانگی                      \n",
      "\n",
      "-\n",
      " ہوا بے خوف و خطر اس ہاتھ پہ سر اس کف پہ جگر          \n",
      "یوں کوئے\n",
      "Decoded sentence:  پھر روئی میں آئی دید درو کو گار  کھا کاب پہ ر       \n",
      "\n",
      "-\n",
      "یں مل کر خاک ہوئے وہ سرمۂ چشم خلق بنی                 \n",
      "جس خار پ\n",
      "Decoded sentence: ہ جان  میں جا ہے ابر نی صبا آیزے                      \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print(input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('seq2seq2.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
